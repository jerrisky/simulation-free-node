nohup: ignoring input
======== ğŸš€ Start Task: SBU_3DFE ========

--- Fold 0 ---
Seed set to 0
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
Using total batch size 64 = 1 x 64
wandb: WARNING The anonymous setting has no effect and will be removed in a future version.
wandb: Tracking run with wandb version 0.23.1
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in logs/wandb/offline-run-20260105_102452-tpz1jwlm
[!] 0 parameters are freezed.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                               â”ƒ Type      â”ƒ Params â”ƒ Mode  â”ƒ FLOPs â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ task_criterion                     â”‚ MSELoss   â”‚      0 â”‚ train â”‚     0 â”‚
â”‚ 1  â”‚ label_ae_criterion                 â”‚ MSELoss   â”‚      0 â”‚ train â”‚     0 â”‚
â”‚ 2  â”‚ odeblock                           â”‚ ODEBlock  â”‚  462 K â”‚ train â”‚     0 â”‚
â”‚ 3  â”‚ odeblock.odefunc                   â”‚ MLPODEFuâ€¦ â”‚  462 K â”‚ train â”‚     0 â”‚
â”‚ 4  â”‚ odeblock.odefunc.fc1               â”‚ Linear    â”‚ 66.0 K â”‚ train â”‚     0 â”‚
â”‚ 5  â”‚ odeblock.odefunc.fc2               â”‚ Linear    â”‚ 66.0 K â”‚ train â”‚     0 â”‚
â”‚ 6  â”‚ odeblock.odefunc.fc3               â”‚ Linear    â”‚ 66.0 K â”‚ train â”‚     0 â”‚
â”‚ 7  â”‚ odeblock.odefunc.non_linearity     â”‚ ReLU      â”‚      0 â”‚ train â”‚     0 â”‚
â”‚ 8  â”‚ odeblock.odefunc.tmlp_blocks       â”‚ ModuleLiâ€¦ â”‚  264 K â”‚ train â”‚     0 â”‚
â”‚ 9  â”‚ odeblock.odefunc.tmlp_blocks.0     â”‚ tMLPBlock â”‚ 66.0 K â”‚ train â”‚     0 â”‚
â”‚ 10 â”‚ odeblock.odefunc.tmlp_blocks.0.fc1 â”‚ Linear    â”‚ 66.0 K â”‚ train â”‚     0 â”‚
â”‚ 11 â”‚ odeblock.odefunc.tmlp_blocks.1     â”‚ tMLPBlock â”‚ 66.0 K â”‚ train â”‚     0 â”‚
â”‚ 12 â”‚ odeblock.odefunc.tmlp_blocks.1.fc1 â”‚ Linear    â”‚ 66.0 K â”‚ train â”‚     0 â”‚
â”‚ 13 â”‚ odeblock.odefunc.tmlp_blocks.2     â”‚ tMLPBlock â”‚ 66.0 K â”‚ train â”‚     0 â”‚
â”‚ 14 â”‚ odeblock.odefunc.tmlp_blocks.2.fc1 â”‚ Linear    â”‚ 66.0 K â”‚ train â”‚     0 â”‚
â”‚ 15 â”‚ odeblock.odefunc.tmlp_blocks.3     â”‚ tMLPBlock â”‚ 66.0 K â”‚ train â”‚     0 â”‚
â”‚ 16 â”‚ odeblock.odefunc.tmlp_blocks.3.fc1 â”‚ Linear    â”‚ 66.0 K â”‚ train â”‚     0 â”‚
â”‚ 17 â”‚ in_projection                      â”‚ Linear    â”‚ 62.5 K â”‚ train â”‚     0 â”‚
â”‚ 18 â”‚ out_projection                     â”‚ Linear    â”‚  1.5 K â”‚ train â”‚     0 â”‚
â”‚ 19 â”‚ label_projection                   â”‚ Linear    â”‚  1.8 K â”‚ train â”‚     0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 528 K                                                         
Non-trainable params: 0                                                         
Total params: 528 K                                                             
Total estimated model params size (MB): 2                                       
Modules in train mode: 20                                                       
Modules in eval mode: 0                                                         
Total FLOPs: 0                                                                  
 Epoch 0: val/avg_imp = -36.2010
 Details: Cheby=0.3015, Clark=2.0129, Canbe=4.8301, KL=12.1160, Cos=0.4057, Inter=0.5320
 Epoch 103: val/avg_imp = -0.2155
 Details: Cheby=0.1415, Clark=0.4267, Canbe=0.9249, KL=0.0878, Cos=0.9147, Inter=0.8336
 Epoch 206: val/avg_imp = -0.2206
 Details: Cheby=0.1416, Clark=0.4275, Canbe=0.9247, KL=0.0895, Cos=0.9148, Inter=0.8336
 Epoch 309: val/avg_imp = -0.2174
 Details: Cheby=0.1419, Clark=0.4268, Canbe=0.9264, KL=0.0881, Cos=0.9143, Inter=0.8334
 Epoch 412: val/avg_imp = -0.2201
 Details: Cheby=0.1415, Clark=0.4283, Canbe=0.9267, KL=0.0890, Cos=0.9146, Inter=0.8333
 Epoch 515: val/avg_imp = -0.2186
 Details: Cheby=0.1416, Clark=0.4280, Canbe=0.9271, KL=0.0885, Cos=0.9145, Inter=0.8332
`Trainer.fit` stopped: `max_steps=10000` reached.
`weights_only` was not set, defaulting to `False`.
[!] Saved last checkpoint at ./logs/last_step=10000.ckpt
Epoch 526/-2 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6/6 0:00:00 â€¢ 0:00:00 91.32it/s v_num: jwlm      
                                                               val/best_avg_imp:
                                                               -0.215           
wandb: 
wandb: Run history:
wandb:           epoch â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:        lr-AdamW â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:      test/Canbe â–‚â–â–†â–‡â–ˆ
wandb:      test/Cheby â–‚â–„â–ˆâ–â–„
wandb:      test/Clark â–â–„â–â–ˆâ–‡
wandb:     test/Cosine â–‡â–ˆâ–â–†â–„
wandb:      test/Inter â–ˆâ–ˆâ–„â–â–
wandb:         test/KL â–â–ˆâ–‚â–†â–„
wandb:  test/avg_imp_1 â–â–â–â–â–
wandb: test/avg_imp_10 â–â–â–â–â–
wandb:             +23 ...
wandb: 
wandb: Run summary:
wandb:           epoch 526
wandb:        lr-AdamW 0.0
wandb:      test/Canbe 0.92714
wandb:      test/Cheby 0.14164
wandb:      test/Clark 0.42798
wandb:     test/Cosine 0.91454
wandb:      test/Inter 0.83323
wandb:         test/KL 0.08845
wandb:  test/avg_imp_1 0
wandb: test/avg_imp_10 0
wandb:             +23 ...
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync logs/wandb/offline-run-20260105_102452-tpz1jwlm
wandb: Find logs at: logs/wandb/offline-run-20260105_102452-tpz1jwlm/logs
Traceback (most recent call last):
  File "/home/ubuntu/zxj/sfnode/auto_run_sfnode.py", line 179, in <module>
    run_experiment(args.dataset, args.device)
  File "/home/ubuntu/zxj/sfnode/auto_run_sfnode.py", line 98, in run_experiment
    print(f"Fold {fold} Imp: {imp:.4f}")
TypeError: unsupported format string passed to dict.__format__
